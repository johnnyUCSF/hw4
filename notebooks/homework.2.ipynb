{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2)\n",
    "\n",
    "### The architecture for my machine learning algorithm I am using is a 68X8X1 neural network. \n",
    "### I chose these numbers as follows:\n",
    "#### 68 = 17 base pairs * 4 (one hot encoding generates 4 nodes per 1 base pair because there are 4 categories)\n",
    "#### 8 as the hidden layer because I am reducing the number of features by a square root roughly to compress information. \n",
    "#### 1 as output layer because we want a prediction score of whether or not it is a true Rap1 binding site. \n",
    "#### For my output layer I will use 1 as true positive Rap1 site, 0 as true negative Rap1 site. \n",
    "\n",
    "### The encoding for my machine learning algorithm is based on one hot encoding.\n",
    "#### For each of the 17 bases, I have encoded each base into 4 bit, with a 1 for each bit indicating that it is true for that base.\n",
    "#### For example, for base 1, A = 1 0 0 0 \n",
    "#### if base 2 were T, the encoding would then be 0 1 0 0\n",
    "#### I then take each 4 bit chunk and concatenate them to get 17 * 4 = 68 bits of information which are used as input to the input nodes. That is why I have 68 input nodes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3)\n",
    "\n",
    "### The number in the positive data set is 137 examples and low in number compared to the reduced set of negatives. Therefore, I added sequences to the positive data set by including reverse complements of the positive sequences. The total of positive training examples is now 274.\n",
    "\n",
    "### There was an overwhelming amount of negative data. I reasoned that if there were sequences in the negative data that closely matched the positive data, then it would slow down the learning process as well as decrease the resolution of the NN. \n",
    "### Therefore I chose to eliminate sequences from the negative data based on similarity to sequences in the positive data. I aligned all the sequences in the positive data and negative data, and filtered out those sequences in the negative data that had high alignment scores to any positive sequence. \n",
    "### This filtered the negative data from ~50,000 examples to ~600 examples.\n",
    "#### Furthermore, I am only taking the first 17 base pairs of the negative examples. I could do a tiling approach in the future.\n",
    "\n",
    "### In this way I have increased the number of positive examples using the supplied information and decreased the amount of negative examples by reducing redundancy and increasing signal. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) \n",
    "\n",
    "### I used the Accuracy definition used in ROC curves. This is calculated as follows:\n",
    "\n",
    "### ( TP + TN ) / ( P + N )\n",
    "\n",
    "### The maximum value of this measurement is 1. Intuitively this measure defines the ratio of true values (pos and neg) versus called values (pos and negative. The closer it is to 1, the better the accuracy of the system is. This value is suitable for our purposes because it takes into account accuracy in both positive and negative directions.\n",
    "\n",
    "### I am using K-Fold cross validation to minimize the effect of bias and maximize accuracy. K fold validation works by splitting the total dataset into multiple chunks and then holding out one chunk at a time and doing iterative training on these k-1 chunks.\n",
    "\n",
    "### I optimized three main parameters using the accuracy metric: \n",
    "\n",
    "#### Number of Iterations\n",
    "#### Step size of the NN algorithm\n",
    "#### Number of Chunks in K-fold cross validation\n",
    "\n",
    "### Based on the plot shown in Fig1, you can see that for most of the step sizes they converge ( in terms of accuracy ) by iteration eight. However, for the best step sizes, they converge early ~ iteration 5.\n",
    "\n",
    "### Therefore I will set iterations = 5 for testing the remaining parameters.\n",
    "\n",
    "### Based on the plot shown in Fig2, you can see that the best step size parameter was step size = 0.2\n",
    "### The best number of chunks for K-fold validation appears to be K = 7.\n",
    "\n",
    "### Shown in Printout A is sample output using iterations=5, step size=0.2, k=7\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
