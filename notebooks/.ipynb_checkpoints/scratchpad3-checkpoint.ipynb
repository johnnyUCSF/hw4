{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import random\n",
    "import math\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "###create test data\n",
    "inputs = ['10000000','01000000','00100000','00010000','00001000','00000100','00000010','00000001']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    sigm = 1. / (1. + np.exp(-x))\n",
    "    if derivative:\n",
    "        return sigm * (1. - sigm)\n",
    "    return sigm\n",
    "\n",
    "def transform_string(outputs):\n",
    "    ##split up \n",
    "    outputs_np = [0.0]*len(outputs)\n",
    "    for i in range(len(outputs)):\n",
    "        outputs_np[i] = float(outputs[i])\n",
    "    outputs_np = np.array(outputs_np)\n",
    "    return(outputs_np)\n",
    "\n",
    "class NeuralNet:\n",
    "    #####creates a one hidden layer NN with output size and input size the same\n",
    "    def __init__(self, size_input, size_hidden, learnrate, lambdarate): \n",
    "        ##define parameters for NN\n",
    "        self.learnrate = float(learnrate)\n",
    "        self.lambdarate = float(lambdarate)\n",
    "        self.size_input = size_input\n",
    "        self.size_hidden = size_hidden\n",
    "        init_factor = 0.1\n",
    "        ##initialize weights \n",
    "        self.weights1 = np.random.rand(size_input,size_hidden)*init_factor\n",
    "        self.weights2 = np.random.rand(size_hidden,size_input)*init_factor\n",
    "        ##initialize biases\n",
    "        self.bias1 = np.random.rand(size_hidden)*init_factor\n",
    "        self.bias2 = np.random.rand(size_input)*init_factor\n",
    "        ##clear delta matrices\n",
    "        self.clear_deltas()\n",
    "    #####clear delta matrices    \n",
    "    def clear_deltas(self):\n",
    "        ##initialize delta matrices which hold delta sum across all examples\n",
    "        self.delta_w1 = np.zeros((self.size_input,self.size_hidden))\n",
    "        self.delta_w2 = np.zeros((self.size_hidden,self.size_input))\n",
    "        #\n",
    "        self.delta_b1 = np.zeros(self.size_hidden)\n",
    "        self.delta_b2 = np.zeros(self.size_input)\n",
    "        ##counter for number of training examples\n",
    "        self.trainset_count = 0\n",
    "    #####propagate forward based on saved input data; note layer 1 = input, layer 2 = hidden layer, layer 3 = output layer\n",
    "    def prop_forward(self,inputs, outputs,):\n",
    "        ##read in inputs and for this training example\n",
    "        self.input_train = inputs\n",
    "        self.output_train = outputs\n",
    "        ##propagate activation functions forward\n",
    "        self.layer2_activation = sigmoid(np.dot(self.input_train,self.weights1)+self.bias1)\n",
    "        self.layer3_activation = sigmoid(np.dot(self.layer2_activation,self.weights2)+self.bias2)\n",
    "        ##update training data counter\n",
    "        self.trainset_count += 1\n",
    "    #####propagate backward\n",
    "    def prop_backward(self):\n",
    "        ##calc derivatives of the weights\n",
    "        self.d_weights2 = np.outer(self.layer2_activation, ((self.output_train - self.layer3_activation) * sigmoid(self.layer3_activation,derivative=True)))\n",
    "        self.d_weights1 = np.outer(self.input_train, (np.dot(self.weights2,(self.output_train - self.layer3_activation) * sigmoid(self.layer3_activation,derivative=True)) * sigmoid(self.layer2_activation,derivative=True)))\n",
    "        ##calc derivatives of the biases\n",
    "        self.d_bias2 = ((self.output_train - self.layer3_activation) * sigmoid(self.layer3_activation,derivative=True))\n",
    "        self.d_bias1 = (np.dot(self.weights2,(self.output_train - self.layer3_activation) * sigmoid(self.layer3_activation,derivative=True)) * sigmoid(self.layer2_activation,derivative=True))\n",
    "        ##add error to delta matrices\n",
    "#         self.delta_w2 += self.d_weights2\n",
    "#         self.delta_w1 += self.d_weights1\n",
    "#         #\n",
    "#         self.delta_b2 += self.d_bias2\n",
    "#         self.delta_b1 += self.d_bias1\n",
    "        ##add error to delta matrices\n",
    "        self.weights2 += self.learnrate*self.d_weights2\n",
    "        self.weights1 += self.learnrate*self.d_weights1\n",
    "        #\n",
    "        self.bias2 += self.learnrate*self.d_bias2\n",
    "        self.bias1 += self.learnrate*self.d_bias1\n",
    "    #####update weights and biases by adding in total delta matrices across all training data\n",
    "    def update_w_b(self):\n",
    "        ###update\n",
    "#         print(self.delta_w2)\n",
    "#         print(self.trainset_count)\n",
    "        print('delta')\n",
    "        print(-self.learnrate*(self.delta_w2/self.trainset_count + self.lambdarate*self.delta_w2))\n",
    "        print('before')\n",
    "        print(self.weights2)\n",
    "        self.weights2 += -self.learnrate*(self.delta_w2/self.trainset_count + self.lambdarate*self.delta_w2)\n",
    "        print('after')\n",
    "        print(self.weights2)\n",
    "        print('self.trainset_count')\n",
    "        print(self.trainset_count)\n",
    "        \n",
    "        self.weights1 += -self.learnrate*(self.delta_w1/self.trainset_count + self.lambdarate*self.delta_w1)\n",
    "        ###update\n",
    "        self.bias2 += -self.learnrate*(self.delta_b2/self.trainset_count)\n",
    "        self.bias1 += -self.learnrate*(self.delta_b1/self.trainset_count)\n",
    "        ###\n",
    "                                          \n",
    "                                          \n",
    "    #####\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.94214452e-02,   4.99561418e-02,   3.64382124e-06,\n",
       "         5.35890581e-05,   3.33408342e-06,   4.36703515e-06,\n",
       "         1.40258627e-01,   9.07202681e-01])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = NeuralNet(8,3,2.0,0.01)\n",
    "iterations = 500\n",
    "for i in range(iterations):\n",
    "    test.clear_deltas()\n",
    "    for item in inputs:\n",
    "        test.prop_forward(transform_string(item),transform_string(item))\n",
    "        test.prop_backward()\n",
    "#     test.update_w_b()\n",
    "# test.prop_forward(transform_string(inputs[0]),transform_string(inputs[0]))\n",
    "test.layer3_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.019421445152070806,\n",
       " 0.049956141836567665,\n",
       " 3.643821238936039e-06,\n",
       " 5.358905812887629e-05,\n",
       " 3.3340834239239305e-06,\n",
       " 4.367035154771994e-06,\n",
       " 0.14025862688598403,\n",
       " 0.9072026809915009]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.layer3_activation.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9359653371907322,\n",
       " 0.0005215970094816714,\n",
       " 0.02957300567989527,\n",
       " 1.9862232352318234e-06,\n",
       " 0.004673477179402312,\n",
       " 2.883314129767275e-08,\n",
       " 0.0025659059296859873,\n",
       " 0.01347377173170292]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.prop_forward(transform_string(inputs[0]),transform_string(inputs[0]))\n",
    "test.layer3_activation.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test.prop_forward(transform_string(inputs[3]),transform_string(inputs[3]))\n",
    "print(test.layer3_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test.prop_forward(transform_string(inputs[3]),transform_string(inputs[3]))\n",
    "print(test.layer3_activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    sigm = 1. / (1. + np.exp(-x))\n",
    "    if derivative:\n",
    "        return sigm * (1. - sigm)\n",
    "    return sigm\n",
    "\n",
    "def init_arch():\n",
    "    ####creates neural net architecture for a 1 hidden layer NN\n",
    "    return\n",
    "\n",
    "def init_deltas(size_input,size_layer):\n",
    "    weight = np.zeros((size_input,size_layer))\n",
    "    bias = np.zeros(size_layer)\n",
    "    return weight,bias\n",
    "\n",
    "class layer:\n",
    "    #####\n",
    "    init_factor = 0.01\n",
    "    #####define size of layer on creation\n",
    "    def __init__(self, size_input, size_layer):\n",
    "        ###read in sizes\n",
    "        self.size_input = size_input\n",
    "        self.size_layer = size_layer\n",
    "        ###init arrays and randomize weights and bias upon creation\n",
    "        self.weight = (np.random.rand(self.size_input,self.size_layer)-0.5)*self.init_factor\n",
    "        self.bias = (np.random.rand(self.size_layer)-0.5)*self.init_factor\n",
    "        self.activation = np.zeros(self.size_layer)\n",
    "    ######propagate forward; takes list of inputs from previous layer\n",
    "    def prop_forward(self,inputs):\n",
    "        if len(inputs) == self.size_input:\n",
    "            ##calculate\n",
    "            z = np.transpose(self.weight).dot(inputs) + self.bias\n",
    "            c = sigmoid(z)\n",
    "            ##save\n",
    "            self.activation = copy.deepcopy(c)\n",
    "        else:\n",
    "            sys.exit('This is the WRONG SIZE')\n",
    "    ######propagate backward; takes list of outputs from previous layer\n",
    "    def prop_backward(self,outputs,outputs_lminus1,lastlayer=False,propd_error=None):\n",
    "        if len(outputs) == self.size_layer:\n",
    "            ###calculate derivative of error per node\n",
    "            if lastlayer == True:\n",
    "                dEdC = outputs-self.activation\n",
    "            else:\n",
    "                dEdC = copy.deepcopy(propd_error)\n",
    "            ###calculate derivative of activation per node\n",
    "            dEdZ = dEdC * sigmoid(self.activation,derivative=True)\n",
    "            ###calculate the derivative of bias per node\n",
    "            dEdB = copy.deepcopy(dEdZ)\n",
    "            ###calculate the derivative of weights\n",
    "            tmp = np.outer(dEdZ,outputs_lminus1)\n",
    "            dEdW = copy.deepcopy(tmp.transpose())\n",
    "            ###calculate error of previous node\n",
    "            error_lminus1 = np.matmul(self.weight,dEdZ)\n",
    "            ###save relevant data\n",
    "            self.dEdB = copy.deepcopy(dEdB)\n",
    "            self.dEdW = copy.deepcopy(dEdW)\n",
    "            ##\n",
    "            self.dEdC = dEdC\n",
    "            self.error_lminus1 = error_lminus1\n",
    "        else:\n",
    "            sys.exit('This is the WRONG SIZE')\n",
    "        return(error_lminus1)\n",
    "    \n",
    "def transform_string(outputs):\n",
    "    ##split up \n",
    "    outputs_np = [0.0]*len(outputs)\n",
    "    for i in range(len(outputs)):\n",
    "        outputs_np[i] = float(outputs[i])\n",
    "    outputs_np = np.array(outputs_np)\n",
    "    return(outputs_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#######define parameters\n",
    "learnrate = 0.2\n",
    "#######define layers\n",
    "layer1 = layer(8,3)\n",
    "layer2 = layer(3,8)\n",
    "######define number iterations\n",
    "iterations = 100\n",
    "#####\n",
    "layer1_error = []\n",
    "layer2_error = []\n",
    "#####\n",
    "delta_weight_1,delta_bias_1 = init_deltas(8,3)\n",
    "delta_weight_2,delta_bias_2 = init_deltas(3,8)\n",
    "#####\n",
    "for i in range(iterations):\n",
    "    ######go through examples\n",
    "    for item in inputs:\n",
    "        ###prop forward\n",
    "        layer1.prop_forward(transform_string(item))\n",
    "        layer2.prop_forward(layer1.activation)\n",
    "        ###prop backward\n",
    "        prop_result = layer2.prop_backward(transform_string(item),layer1.activation,True)\n",
    "        prop_result = layer1.prop_backward(prop_result,transform_string(item),False,layer2.error_lminus1)\n",
    "        \n",
    "        ###\n",
    "        print('l2error')\n",
    "        print(layer2.error_lminus1)\n",
    "        ###add delta term\n",
    "        delta_bias_1 = delta_bias_1 + layer1.dEdB\n",
    "        delta_bias_2 = delta_bias_2 + layer2.dEdB\n",
    "        delta_weight_1 = delta_weight_1 + layer1.dEdW\n",
    "        delta_weight_2 = delta_weight_2 + layer2.dEdW\n",
    "    ######update terms\n",
    "    layer1.bias = layer1.bias - learnrate*(delta_bias_1/float(len(inputs)))\n",
    "    layer2.bias = layer2.bias - learnrate*(delta_bias_2/float(len(inputs)))\n",
    "    layer1.weight = layer1.weight - learnrate*(delta_weight_1/float(len(inputs)))\n",
    "    layer2.weight = layer2.weight - learnrate*(delta_weight_2/float(len(inputs)))\n",
    "    #####check\n",
    "    layer2_error.append(np.sum(layer2.dEdC))\n",
    "    layer1_error.append(np.sum(layer1.dEdC))\n",
    "    #####2\n",
    "#     print(layer2.dEdW)\n",
    "######output for input 1\n",
    "###prop forward\n",
    "layer1.prop_forward(transform_string(inputs[0]))\n",
    "layer2.prop_forward(layer1.activation)\n",
    "print(layer2.activation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(layer1_error)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layer2.activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "layer2.dEdC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.plot(layer2_error)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
